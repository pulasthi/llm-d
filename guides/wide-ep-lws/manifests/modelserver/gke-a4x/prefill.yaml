apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
  labels:
    app: wide-ep-llm-d
    llm-d.ai/inferenceServing: "true"
    llm-d.ai/model: DeepSeek-R1-0528
    llm-d.ai/role: prefill
  name: wide-ep-llm-d-prefill
spec:
  leaderWorkerTemplate:
    restartPolicy: None
    size: 4
    workerTemplate:
      metadata:
        annotations:
          networking.gke.io/default-interface: eth0
          networking.gke.io/interfaces: |
            [
              {"interfaceName":"eth0","network":"default"},
              {"interfaceName":"eth2","network":"rdma-0"},
              {"interfaceName":"eth3","network":"rdma-1"},
              {"interfaceName":"eth4","network":"rdma-2"},
              {"interfaceName":"eth5","network":"rdma-3"}
            ]
        labels:
          app: wide-ep-llm-d
          llm-d.ai/inferenceServing: "true"
          llm-d.ai/model: DeepSeek-R1-0528
          llm-d.ai/role: prefill
      spec:
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: wide-ep-llm-d
              matchLabelKeys:
              - llm-d.ai/role
              topologyKey: cloud.google.com/gce-topology-block
            - labelSelector:
                matchLabels:
                  app: wide-ep-llm-d
              matchLabelKeys:
              - llm-d.ai/role
              topologyKey: cloud.google.com/gce-topology-subblock
        tolerations:
        - effect: NoSchedule
          key: kubernetes.io/arch
          value: arm64

        containers:
        - args:
          - |-
            # Clear /dev/shm on start to prevent running out of space when crashes occur
            # https://github.com/llm-d/llm-d/issues/352
            find /dev/shm -type f -delete

            #################
            # RUN vLLM prefill worker
            #################
            START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))

            # --data-parallel-hybrid-lb: Use exernal load balancing across nodes, and internal load balancing within a node
            # --enable-expert-parallel: Use TPxDP in attention, EP in MoE layers
            # --async-scheduling: Reduce white space between engine steps
            # --enable-dbo: Dual batch overlap (DBO) overlaps compute with collective communication.
            # --enable-eplb:  Expert-parallel load balancing reduces EP load imbalance by replicating heavily-used experts Performance-memory tradeoff: on DeepSeekV3 eplb uses an extra 2GB per redundant expert per GPU Divisibility constraint: num_routed_experts (256 for DSv3) + num_redundant_experts must be divisible by the number of GPUs.

            exec vllm serve \
              deepseek-ai/DeepSeek-R1-0528 \
              --port 8000 \
              --trust-remote-code \
              --disable-uvicorn-access-log \
              --data-parallel-hybrid-lb \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $((LWS_GROUP_SIZE * DP_SIZE_LOCAL)) \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address ${LWS_LEADER_ADDRESS} \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --kv_transfer_config '{"kv_connector":"NixlConnector",
                                      "kv_role":"kv_both",
                                      "kv_load_failure_policy":"fail"}' \
              --async-scheduling \
              --enable-dbo \
              --dbo-prefill-token-threshold 32 \
              --enable-eplb \
              --eplb-config '{"window_size":"1000",
                              "step_interval":"3000",
                              "num_redundant_experts":"32",
                              "log_balancedness":"False"}' \
              --gpu-memory-utilization 0.75
          command:
          - /bin/bash
          - -c
          env:
          - name: DP_SIZE_LOCAL
            value: "4"
          - name: TP_SIZE
            value: "1"
          - name: TRITON_LIBCUDA_PATH
            value: /usr/lib64
          - name: VLLM_SKIP_P2P_CHECK
            value: "1"
          - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
            value: "1"
          - name: VLLM_USE_DEEP_GEMM
            value: "1"
          - name: VLLM_ALL2ALL_BACKEND
            value: deepep_high_throughput
          - name: NVIDIA_GDRCOPY
            value: enabled
          - name: NVSHMEM_REMOTE_TRANSPORT
            value: ibgda
          - name: NVSHMEM_IB_ENABLE_IBGDA
            value: "true"
          - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
            value: eth0
          - name: GLOO_SOCKET_IFNAME
            value: eth0
          - name: NCCL_SOCKET_IFNAME
            value: eth0
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: CUDA_CACHE_PATH
            value: /var/cache/vllm/cuda
          - name: CCACHE_DIR
            value: /var/cache/vllm/ccache
          - name: VLLM_CACHE_ROOT
            value: /var/cache/vllm/vllm
          - name: FLASHINFER_WORKSPACE_BASE
            value: /var/cache/vllm/flashinfer
          - name: HF_HUB_CACHE
            value: /var/cache/huggingface
          - name: BASH_ENV
            value: /usr/local/gib/scripts/set_nccl_env.sh
          - name: NVSHMEM_DISABLED_GDRCOPY
            value: "true"
          - name: NCCL_DEBUG
            value: INFO
          image: ghcr.io/llm-d/llm-d-cuda-dev:pr-571
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: metrics
            periodSeconds: 30
            timeoutSeconds: 5
          name: vllm
          ports:
          - containerPort: 8000
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /v1/models
              port: metrics
            periodSeconds: 10
            timeoutSeconds: 5
          resources:
            limits:
              ephemeral-storage: 1Ti
              memory: 512Gi
              nvidia.com/gpu: "4"
            requests:
              cpu: 32
              ephemeral-storage: 1Ti
              memory: 512Gi
              nvidia.com/gpu: "4"
          securityContext:
            capabilities:
              add:
              - IPC_LOCK
              - SYS_RAWIO
            privileged: true
            runAsGroup: 0
            runAsUser: 0
          startupProbe:
            failureThreshold: 2700
            httpGet:
              path: /health
              port: metrics
            initialDelaySeconds: 0
            periodSeconds: 1
            timeoutSeconds: 5
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          - mountPath: /var/cache/huggingface
            name: hf-cache
          - mountPath: /var/cache/vllm
            name: jit-cache
          - mountPath: /usr/local/gib
            name: gib
        serviceAccountName: deepseek-r1
        volumes:
        - emptyDir:
            medium: Memory
            sizeLimit: 2Gi
          name: dshm
        - hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-hf-cache/
            type: DirectoryOrCreate
          name: hf-cache
        - hostPath:
            path: /mnt/stateful_partition/kube-ephemeral-ssd/shared_disk/vllm-jit-cache/
            type: DirectoryOrCreate
          name: jit-cache
        - hostPath:
            path: /home/kubernetes/bin/gib
            type: ""
          name: gib
